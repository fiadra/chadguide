{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Flight Router Benchmarking Suite\n",
    "\n",
    "**Three separate benchmarks** for accurate performance measurement:\n",
    "\n",
    "| Benchmark | What it measures | API calls | Purpose |\n",
    "|-----------|------------------|-----------|----------|\n",
    "| **Part A: Routing** | Pure algorithm performance | None | Algorithm optimization |\n",
    "| **Part B: Validation** | API latency only | Yes (fixed routes) | API capacity planning |\n",
    "| **Part C: Full Pipeline** | End-to-end performance | Yes | Realistic user experience |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import itertools\n",
    "import statistics\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Set, Dict, Any, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Setup path\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Core imports\n",
    "from src.flight_router.application import FindOptimalRoutes\n",
    "from src.flight_router.services import RouteValidationService\n",
    "from src.flight_router.adapters.validators import DuffelOfferValidator\n",
    "from src.flight_router.schemas.validation import ValidationConfig\n",
    "\n",
    "# Configuration\n",
    "DEMO_DB = Path(\"data/demo_flights.db\")\n",
    "DEPARTURE_DATE = datetime(2026, 7, 13)\n",
    "RETURN_DATE = datetime(2026, 7, 19)\n",
    "\n",
    "# API Token for validation\n",
    "DUFFEL_API_TOKEN = os.environ.get(\"DUFFEL_API_TOKEN\", \"\")\n",
    "HAS_API_TOKEN = bool(DUFFEL_API_TOKEN)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Database: {DEMO_DB}\")\n",
    "print(f\"Dates: {DEPARTURE_DATE.date()} to {RETURN_DATE.date()}\")\n",
    "if HAS_API_TOKEN:\n",
    "    print(f\"API Token: {DUFFEL_API_TOKEN[:8]}...{DUFFEL_API_TOKEN[-4:]}\")\n",
    "else:\n",
    "    print(\"No API token - Parts B and C will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available destinations\n",
    "ORIGIN = \"WAW\"\n",
    "\n",
    "with FindOptimalRoutes(db_path=DEMO_DB) as router:\n",
    "    all_airports = router.get_available_airports()\n",
    "    reachable = {city for city in all_airports if router.has_route(ORIGIN, city)}\n",
    "\n",
    "CANDIDATE_HUBS = ['LHR', 'CDG', 'FRA', 'AMS', 'MUC', 'FCO', 'VIE', 'ZRH', 'BRU', 'BCN', 'MAD', 'CPH']\n",
    "DESTINATIONS = [h for h in CANDIDATE_HUBS if h in reachable]\n",
    "\n",
    "print(f\"Origin: {ORIGIN}\")\n",
    "print(f\"Available destinations: {DESTINATIONS[:8]}...\")\n",
    "print(f\"Total: {len(DESTINATIONS)} destinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parta-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Routing Benchmark\n",
    "\n",
    "**Measures**: Pure routing algorithm performance (no API calls)\n",
    "\n",
    "**Why separate**: Routing has O(2^k) complexity due to power-set state space.\n",
    "The algorithm tracks which cities are visited (2^k subsets), not visit order (k! permutations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RoutingBenchmarkConfig:\n",
    "    \"\"\"Configuration for routing benchmark.\"\"\"\n",
    "    origin: str = \"WAW\"\n",
    "    max_k: int = 3              # Max destinations\n",
    "    combos_per_k: int = 3       # Destination combinations per k\n",
    "    runs_per_combo: int = 3     # Runs per combination\n",
    "    warmup_runs: int = 1        # Warmup (excluded from results)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RoutingResult:\n",
    "    \"\"\"Result of a single routing benchmark run.\"\"\"\n",
    "    k: int\n",
    "    destinations: tuple\n",
    "    run_index: int\n",
    "    time_ms: float\n",
    "    num_routes: int\n",
    "\n",
    "\n",
    "routing_config = RoutingBenchmarkConfig(\n",
    "    max_k=3,\n",
    "    combos_per_k=3,\n",
    "    runs_per_combo=3,\n",
    ")\n",
    "\n",
    "print(\"Routing Benchmark Configuration:\")\n",
    "print(f\"  Max k: {routing_config.max_k}\")\n",
    "print(f\"  Combinations per k: {routing_config.combos_per_k}\")\n",
    "print(f\"  Runs per combination: {routing_config.runs_per_combo}\")\n",
    "\n",
    "total_runs = sum(\n",
    "    min(routing_config.combos_per_k, len(list(itertools.combinations(DESTINATIONS[:k+2], k))))\n",
    "    * routing_config.runs_per_combo\n",
    "    for k in range(1, routing_config.max_k + 1)\n",
    ")\n",
    "print(f\"  Total runs: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_routing_benchmark(config: RoutingBenchmarkConfig, destinations: List[str]) -> List[RoutingResult]:\n",
    "    \"\"\"Run pure routing benchmark (no API calls).\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with FindOptimalRoutes(db_path=DEMO_DB) as router:\n",
    "        for k in range(1, config.max_k + 1):\n",
    "            if k > len(destinations):\n",
    "                print(f\"Skipping k={k}: not enough destinations\")\n",
    "                continue\n",
    "            \n",
    "            # Get destination combinations\n",
    "            all_combos = list(itertools.combinations(destinations[:k+2], k))\n",
    "            combos = all_combos[:config.combos_per_k]\n",
    "            \n",
    "            print(f\"\\nk={k}: {len(combos)} combinations x {config.runs_per_combo} runs\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for combo in combos:\n",
    "                dest_set = set(combo)\n",
    "                \n",
    "                # Warmup\n",
    "                for _ in range(config.warmup_runs):\n",
    "                    router.search(\n",
    "                        origin=config.origin,\n",
    "                        destinations=dest_set,\n",
    "                        departure_date=DEPARTURE_DATE,\n",
    "                        return_date=RETURN_DATE,\n",
    "                    )\n",
    "                \n",
    "                # Benchmark runs\n",
    "                combo_times = []\n",
    "                num_routes = 0\n",
    "                \n",
    "                for run_idx in range(config.runs_per_combo):\n",
    "                    start = time.perf_counter()\n",
    "                    routes = router.search(\n",
    "                        origin=config.origin,\n",
    "                        destinations=dest_set,\n",
    "                        departure_date=DEPARTURE_DATE,\n",
    "                        return_date=RETURN_DATE,\n",
    "                    )\n",
    "                    elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "                    \n",
    "                    num_routes = len(routes)\n",
    "                    combo_times.append(elapsed_ms)\n",
    "                    \n",
    "                    results.append(RoutingResult(\n",
    "                        k=k,\n",
    "                        destinations=combo,\n",
    "                        run_index=run_idx,\n",
    "                        time_ms=elapsed_ms,\n",
    "                        num_routes=num_routes,\n",
    "                    ))\n",
    "                \n",
    "                avg_time = statistics.mean(combo_times)\n",
    "                std_time = statistics.stdev(combo_times) if len(combo_times) > 1 else 0\n",
    "                print(f\"  {combo}: {avg_time:.0f}ms (+/-{std_time:.0f}ms), {num_routes} routes\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Starting Routing Benchmark...\")\n",
    "print(\"=\" * 60)\n",
    "routing_results = run_routing_benchmark(routing_config, DESTINATIONS)\n",
    "print(f\"\\nCompleted {len(routing_results)} routing benchmark runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze routing results\n",
    "routing_stats = {}\n",
    "by_k = {}\n",
    "for r in routing_results:\n",
    "    if r.k not in by_k:\n",
    "        by_k[r.k] = []\n",
    "    by_k[r.k].append(r)\n",
    "\n",
    "for k, runs in sorted(by_k.items()):\n",
    "    times = [r.time_ms for r in runs]\n",
    "    routes = [r.num_routes for r in runs]\n",
    "    routing_stats[k] = {\n",
    "        'mean': statistics.mean(times),\n",
    "        'std': statistics.stdev(times) if len(times) > 1 else 0,\n",
    "        'min': min(times),\n",
    "        'max': max(times),\n",
    "        'avg_routes': statistics.mean(routes),\n",
    "        'n': len(times),\n",
    "    }\n",
    "\n",
    "print(\"Routing Benchmark Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'k':>3} | {'Mean (ms)':>12} | {'Std':>8} | {'Min':>8} | {'Max':>8} | {'Routes':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for k, s in routing_stats.items():\n",
    "    print(f\"{k:>3} | {s['mean']:>12.0f} | {s['std']:>8.0f} | {s['min']:>8.0f} | {s['max']:>8.0f} | {s['avg_routes']:>8.1f}\")\n",
    "\n",
    "# Scaling analysis\n",
    "print(\"\\nScaling Analysis:\")\n",
    "k_values = sorted(routing_stats.keys())\n",
    "for i in range(1, len(k_values)):\n",
    "    k_prev, k_curr = k_values[i-1], k_values[i]\n",
    "    ratio = routing_stats[k_curr]['mean'] / routing_stats[k_prev]['mean']\n",
    "    print(f\"  k={k_prev} -> k={k_curr}: {ratio:.2f}x increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partb-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Validation Benchmark\n",
    "\n",
    "**Measures**: Pure API latency (fixed routes, no routing overhead)\n",
    "\n",
    "**Why separate**: \n",
    "- Uses fixed, pre-computed routes to isolate API latency\n",
    "- Strict pacing (1 request/second) to avoid rate limiting variance\n",
    "- Measures what matters for capacity planning: API response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationBenchmarkConfig:\n",
    "    \"\"\"Configuration for validation-only benchmark.\"\"\"\n",
    "    num_routes_to_validate: int = 5     # Routes to validate\n",
    "    runs_per_route: int = 2             # Repeat each route validation\n",
    "    requests_per_minute: float = 50.0   # Strict pacing to avoid 429s\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a single validation benchmark run.\"\"\"\n",
    "    route_index: int\n",
    "    run_index: int\n",
    "    num_segments: int\n",
    "    validation_time_ms: float\n",
    "    status: str\n",
    "    confidence: float\n",
    "    is_bookable: bool\n",
    "\n",
    "\n",
    "validation_config = ValidationBenchmarkConfig(\n",
    "    num_routes_to_validate=5,\n",
    "    runs_per_route=2,\n",
    "    requests_per_minute=50.0,\n",
    ")\n",
    "\n",
    "print(\"Validation Benchmark Configuration:\")\n",
    "print(f\"  Routes to validate: {validation_config.num_routes_to_validate}\")\n",
    "print(f\"  Runs per route: {validation_config.runs_per_route}\")\n",
    "print(f\"  Rate limit: {validation_config.requests_per_minute} req/min\")\n",
    "print(f\"  Min interval: {60.0 / validation_config.requests_per_minute:.2f}s between requests\")\n",
    "\n",
    "if not HAS_API_TOKEN:\n",
    "    print(\"\\nWARNING: No API token - this benchmark will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_validation_benchmark(\n",
    "    config: ValidationBenchmarkConfig,\n",
    ") -> List[ValidationResult]:\n",
    "    \"\"\"Run validation-only benchmark with strict pacing.\"\"\"\n",
    "    \n",
    "    if not HAS_API_TOKEN:\n",
    "        print(\"Skipping: No API token\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    min_interval = 60.0 / config.requests_per_minute\n",
    "    \n",
    "    # Get some routes to validate (using k=2 for variety)\n",
    "    print(\"Getting routes to validate...\")\n",
    "    with FindOptimalRoutes(db_path=DEMO_DB) as router:\n",
    "        test_routes = router.search(\n",
    "            origin=ORIGIN,\n",
    "            destinations={'LHR', 'CDG'},\n",
    "            departure_date=DEPARTURE_DATE,\n",
    "            return_date=RETURN_DATE,\n",
    "        )[:config.num_routes_to_validate]\n",
    "    \n",
    "    if not test_routes:\n",
    "        print(\"No routes found to validate\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(test_routes)} routes to benchmark\")\n",
    "    \n",
    "    # Setup validator\n",
    "    validator = DuffelOfferValidator(api_token=DUFFEL_API_TOKEN)\n",
    "    validator_service = RouteValidationService(validator)\n",
    "    \n",
    "    print(f\"\\nValidating {len(test_routes)} routes x {config.runs_per_route} runs\")\n",
    "    print(f\"Pacing: {min_interval:.1f}s between validation requests\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    last_request_time = 0.0\n",
    "    \n",
    "    for route_idx, route in enumerate(test_routes):\n",
    "        num_segments = len(route.segments)\n",
    "        \n",
    "        for run_idx in range(config.runs_per_route):\n",
    "            # Strict pacing: wait before each validation\n",
    "            now = time.perf_counter()\n",
    "            if last_request_time > 0:\n",
    "                elapsed = now - last_request_time\n",
    "                if elapsed < min_interval:\n",
    "                    wait_time = min_interval - elapsed\n",
    "                    await asyncio.sleep(wait_time)\n",
    "            \n",
    "            # Validate and measure\n",
    "            start = time.perf_counter()\n",
    "            validated = await validator_service.validate_routes(\n",
    "                routes=[route],\n",
    "                departure_date=DEPARTURE_DATE.date(),\n",
    "                validate_top_n=1,\n",
    "            )\n",
    "            elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "            last_request_time = time.perf_counter()\n",
    "            \n",
    "            vr = validated[0] if validated else None\n",
    "            status = vr.validation.status.value if vr and vr.validation else \"N/A\"\n",
    "            confidence = vr.validation.average_confidence if vr and vr.validation else 0.0\n",
    "            is_bookable = vr.is_bookable if vr else False\n",
    "            \n",
    "            results.append(ValidationResult(\n",
    "                route_index=route_idx,\n",
    "                run_index=run_idx,\n",
    "                num_segments=num_segments,\n",
    "                validation_time_ms=elapsed_ms,\n",
    "                status=status,\n",
    "                confidence=confidence,\n",
    "                is_bookable=is_bookable,\n",
    "            ))\n",
    "            \n",
    "            print(f\"  Route {route_idx+1} run {run_idx+1}: {elapsed_ms:.0f}ms, status={status}, confidence={confidence:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if HAS_API_TOKEN:\n",
    "    print(\"Starting Validation Benchmark...\")\n",
    "    print(\"=\" * 60)\n",
    "    validation_results = await run_validation_benchmark(validation_config)\n",
    "    print(f\"\\nCompleted {len(validation_results)} validation benchmark runs\")\n",
    "else:\n",
    "    print(\"Validation benchmark skipped (no API token)\")\n",
    "    validation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_results:\n",
    "    times = [r.validation_time_ms for r in validation_results]\n",
    "    validation_stats = {\n",
    "        'mean': statistics.mean(times),\n",
    "        'std': statistics.stdev(times) if len(times) > 1 else 0,\n",
    "        'min': min(times),\n",
    "        'max': max(times),\n",
    "        'p50': statistics.median(times),\n",
    "        'n': len(times),\n",
    "        'bookable_rate': sum(1 for r in validation_results if r.is_bookable) / len(validation_results) * 100,\n",
    "        'avg_confidence': statistics.mean(r.confidence for r in validation_results),\n",
    "    }\n",
    "    \n",
    "    cv = (validation_stats['std'] / validation_stats['mean'] * 100) if validation_stats['mean'] > 0 else 0\n",
    "    \n",
    "    print(\"Validation Benchmark Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Samples: {validation_stats['n']}\")\n",
    "    print(f\"Mean: {validation_stats['mean']:.0f}ms\")\n",
    "    print(f\"Std dev: {validation_stats['std']:.0f}ms\")\n",
    "    print(f\"Min: {validation_stats['min']:.0f}ms\")\n",
    "    print(f\"Max: {validation_stats['max']:.0f}ms\")\n",
    "    print(f\"Median (P50): {validation_stats['p50']:.0f}ms\")\n",
    "    print(f\"Coefficient of Variation: {cv:.1f}%\")\n",
    "    print(f\"\\nBookable rate: {validation_stats['bookable_rate']:.1f}%\")\n",
    "    print(f\"Avg confidence: {validation_stats['avg_confidence']:.1f}%\")\n",
    "    \n",
    "    if cv < 30:\n",
    "        print(\"\\nVariance is LOW - results are consistent\")\n",
    "    else:\n",
    "        print(\"\\nVariance is HIGH - consider stricter pacing\")\n",
    "else:\n",
    "    validation_stats = None\n",
    "    print(\"No validation results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partc-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Full Pipeline Benchmark\n",
    "\n",
    "**Measures**: Realistic end-to-end performance (routing + validation)\n",
    "\n",
    "**When to use**: Understanding total user-facing latency for production planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineBenchmarkConfig:\n",
    "    \"\"\"Configuration for full pipeline benchmark.\"\"\"\n",
    "    origin: str = \"WAW\"\n",
    "    max_k: int = 2                      # Keep small due to API time\n",
    "    combos_per_k: int = 2               # Fewer combinations\n",
    "    runs_per_combo: int = 2             # Fewer runs\n",
    "    validate_top_n: int = 2             # Validate top N routes\n",
    "    requests_per_minute: float = 50.0   # Conservative pacing\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResult:\n",
    "    \"\"\"Result of a full pipeline benchmark run.\"\"\"\n",
    "    k: int\n",
    "    destinations: tuple\n",
    "    run_index: int\n",
    "    routing_time_ms: float\n",
    "    validation_time_ms: float\n",
    "    num_routes_found: int\n",
    "    num_routes_validated: int\n",
    "    num_routes_bookable: int\n",
    "    \n",
    "    @property\n",
    "    def total_time_ms(self) -> float:\n",
    "        return self.routing_time_ms + self.validation_time_ms\n",
    "\n",
    "\n",
    "pipeline_config = PipelineBenchmarkConfig(\n",
    "    max_k=2,\n",
    "    combos_per_k=2,\n",
    "    runs_per_combo=2,\n",
    "    validate_top_n=2,\n",
    "    requests_per_minute=50.0,\n",
    ")\n",
    "\n",
    "print(\"Full Pipeline Benchmark Configuration:\")\n",
    "print(f\"  Max k: {pipeline_config.max_k}\")\n",
    "print(f\"  Combinations per k: {pipeline_config.combos_per_k}\")\n",
    "print(f\"  Runs per combination: {pipeline_config.runs_per_combo}\")\n",
    "print(f\"  Validate top N: {pipeline_config.validate_top_n}\")\n",
    "print(f\"  Rate limit: {pipeline_config.requests_per_minute} req/min\")\n",
    "\n",
    "if not HAS_API_TOKEN:\n",
    "    print(\"\\nWARNING: No API token - this benchmark will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline_benchmark(\n",
    "    config: PipelineBenchmarkConfig,\n",
    "    destinations: List[str],\n",
    ") -> List[PipelineResult]:\n",
    "    \"\"\"Run full pipeline benchmark (routing + validation).\"\"\"\n",
    "    \n",
    "    if not HAS_API_TOKEN:\n",
    "        print(\"Skipping: No API token\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    min_interval = 60.0 / config.requests_per_minute\n",
    "    last_request_time = 0.0\n",
    "    \n",
    "    validator = DuffelOfferValidator(api_token=DUFFEL_API_TOKEN)\n",
    "    validator_service = RouteValidationService(validator)\n",
    "    \n",
    "    with FindOptimalRoutes(db_path=DEMO_DB) as router:\n",
    "        for k in range(1, config.max_k + 1):\n",
    "            if k > len(destinations):\n",
    "                continue\n",
    "            \n",
    "            combos = list(itertools.combinations(destinations[:k+2], k))[:config.combos_per_k]\n",
    "            \n",
    "            print(f\"\\nk={k}: {len(combos)} combinations x {config.runs_per_combo} runs\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for combo in combos:\n",
    "                dest_set = set(combo)\n",
    "                \n",
    "                for run_idx in range(config.runs_per_combo):\n",
    "                    # Phase 1: Routing\n",
    "                    routing_start = time.perf_counter()\n",
    "                    routes = router.search(\n",
    "                        origin=config.origin,\n",
    "                        destinations=dest_set,\n",
    "                        departure_date=DEPARTURE_DATE,\n",
    "                        return_date=RETURN_DATE,\n",
    "                    )\n",
    "                    routing_time_ms = (time.perf_counter() - routing_start) * 1000\n",
    "                    \n",
    "                    # Phase 2: Validation with pacing\n",
    "                    now = time.perf_counter()\n",
    "                    if last_request_time > 0:\n",
    "                        elapsed = now - last_request_time\n",
    "                        if elapsed < min_interval:\n",
    "                            await asyncio.sleep(min_interval - elapsed)\n",
    "                    \n",
    "                    validation_start = time.perf_counter()\n",
    "                    validated = await validator_service.validate_routes(\n",
    "                        routes=routes,\n",
    "                        departure_date=DEPARTURE_DATE.date(),\n",
    "                        validate_top_n=config.validate_top_n,\n",
    "                    )\n",
    "                    validation_time_ms = (time.perf_counter() - validation_start) * 1000\n",
    "                    last_request_time = time.perf_counter()\n",
    "                    \n",
    "                    num_validated = sum(1 for v in validated if v.is_validated)\n",
    "                    num_bookable = sum(1 for v in validated if v.is_bookable)\n",
    "                    \n",
    "                    results.append(PipelineResult(\n",
    "                        k=k,\n",
    "                        destinations=combo,\n",
    "                        run_index=run_idx,\n",
    "                        routing_time_ms=routing_time_ms,\n",
    "                        validation_time_ms=validation_time_ms,\n",
    "                        num_routes_found=len(routes),\n",
    "                        num_routes_validated=num_validated,\n",
    "                        num_routes_bookable=num_bookable,\n",
    "                    ))\n",
    "                    \n",
    "                    print(f\"  {combo} run {run_idx+1}: routing={routing_time_ms:.0f}ms, \"\n",
    "                          f\"validation={validation_time_ms:.0f}ms, total={routing_time_ms + validation_time_ms:.0f}ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if HAS_API_TOKEN:\n",
    "    print(\"Starting Full Pipeline Benchmark...\")\n",
    "    print(\"=\" * 60)\n",
    "    pipeline_results = await run_pipeline_benchmark(pipeline_config, DESTINATIONS)\n",
    "    print(f\"\\nCompleted {len(pipeline_results)} pipeline benchmark runs\")\n",
    "else:\n",
    "    print(\"Full pipeline benchmark skipped (no API token)\")\n",
    "    pipeline_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_results:\n",
    "    pipeline_stats = {}\n",
    "    by_k = {}\n",
    "    for r in pipeline_results:\n",
    "        if r.k not in by_k:\n",
    "            by_k[r.k] = []\n",
    "        by_k[r.k].append(r)\n",
    "    \n",
    "    for k, runs in sorted(by_k.items()):\n",
    "        routing_times = [r.routing_time_ms for r in runs]\n",
    "        validation_times = [r.validation_time_ms for r in runs]\n",
    "        total_times = [r.total_time_ms for r in runs]\n",
    "        \n",
    "        pipeline_stats[k] = {\n",
    "            'routing_mean': statistics.mean(routing_times),\n",
    "            'validation_mean': statistics.mean(validation_times),\n",
    "            'total_mean': statistics.mean(total_times),\n",
    "            'total_std': statistics.stdev(total_times) if len(total_times) > 1 else 0,\n",
    "            'avg_routes': statistics.mean(r.num_routes_found for r in runs),\n",
    "            'avg_bookable': statistics.mean(r.num_routes_bookable for r in runs),\n",
    "        }\n",
    "    \n",
    "    print(\"Full Pipeline Benchmark Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'k':>3} | {'Routing (ms)':>14} | {'Validation (ms)':>16} | {'Total (ms)':>14} | {'Bookable':>8}\")\n",
    "    print(\"-\" * 80)\n",
    "    for k, s in pipeline_stats.items():\n",
    "        print(f\"{k:>3} | {s['routing_mean']:>14.0f} | {s['validation_mean']:>16.0f} | \"\n",
    "              f\"{s['total_mean']:>14.0f} | {s['avg_bookable']:>8.1f}\")\n",
    "    \n",
    "    print(\"\\nTime breakdown at max k:\")\n",
    "    max_k = max(pipeline_stats.keys())\n",
    "    s = pipeline_stats[max_k]\n",
    "    total = s['total_mean']\n",
    "    print(f\"  Routing:    {s['routing_mean']:>8.0f}ms ({s['routing_mean']/total*100:.1f}%)\")\n",
    "    print(f\"  Validation: {s['validation_mean']:>8.0f}ms ({s['validation_mean']/total*100:.1f}%)\")\n",
    "else:\n",
    "    pipeline_stats = None\n",
    "    print(\"No pipeline results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity fitting function\n",
    "def fit_exponential(k_vals, times):\n",
    "    \"\"\"\n",
    "    Fit T(k) = c × b^k to data using log-linear regression.\n",
    "    \n",
    "    Returns (c, b, r_squared) where:\n",
    "    - c: coefficient\n",
    "    - b: base of exponential (should be ~2.0 for O(2^k))\n",
    "    - r_squared: goodness of fit in log space\n",
    "    \"\"\"\n",
    "    k_arr = np.array(k_vals)\n",
    "    log_times = np.log(times)\n",
    "    # Linear fit in log space: log(T) = log(c) + k × log(b)\n",
    "    slope, intercept = np.polyfit(k_arr, log_times, 1)\n",
    "    b = np.exp(slope)   # Base of exponential\n",
    "    c = np.exp(intercept)  # Coefficient\n",
    "    \n",
    "    # R² in log space\n",
    "    log_pred = intercept + slope * k_arr\n",
    "    ss_res = np.sum((log_times - log_pred) ** 2)\n",
    "    ss_tot = np.sum((log_times - np.mean(log_times)) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    return c, b, r_squared\n",
    "\n",
    "# Fit the complexity model\n",
    "k_vals = sorted(routing_stats.keys())\n",
    "means = [routing_stats[k]['mean'] for k in k_vals]\n",
    "stds = [routing_stats[k]['std'] for k in k_vals]\n",
    "\n",
    "c_fit, b_fit, r_sq = fit_exponential(k_vals, means)\n",
    "\n",
    "# Determine if O(2^k) is confirmed\n",
    "is_exp_2 = (1.7 < b_fit < 2.4) and (r_sq > 0.95)\n",
    "deviation_from_2 = abs(b_fit - 2.0) / 2.0 * 100\n",
    "\n",
    "print(\"Complexity Analysis:\")\n",
    "print(f\"  Fitted model: T(k) = {c_fit:.0f} × {b_fit:.2f}^k\")\n",
    "print(f\"  Fitted base: {b_fit:.3f} (deviation from 2.0: {deviation_from_2:.1f}%)\")\n",
    "print(f\"  R² (log space): {r_sq:.4f}\")\n",
    "print(f\"  Expected for O(2^k): base ≈ 2.0, R² ≈ 1.0\")\n",
    "print(f\"  Conclusion: {'O(2^k) confirmed' if is_exp_2 else 'Check fit details'}\")\n",
    "\n",
    "# Determine what to plot based on available data\n",
    "has_validation = bool(validation_results)\n",
    "has_pipeline = bool(pipeline_results)\n",
    "\n",
    "if has_validation and has_pipeline:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "elif has_pipeline:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes = [[axes[0], axes[1]], [None, None]]\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes = [[axes[0], axes[1]], [None, None]]\n",
    "\n",
    "fig.suptitle('Flight Router Benchmark Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Routing benchmark results\n",
    "ax1 = axes[0][0]\n",
    "ax1.bar(k_vals, means, yerr=stds, capsize=5, color='#2196F3', alpha=0.8)\n",
    "ax1.set_xlabel('Number of Destinations (k)')\n",
    "ax1.set_ylabel('Time (ms)')\n",
    "ax1.set_title('Part A: Routing Performance')\n",
    "ax1.set_xticks(k_vals)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Routing scaling (log) with theoretical fit\n",
    "ax2 = axes[0][1]\n",
    "\n",
    "# Observed data points\n",
    "ax2.semilogy(k_vals, means, 'o', color='#2196F3', markersize=12, label='Observed', zorder=3)\n",
    "\n",
    "# Theoretical fit line\n",
    "k_theory = np.linspace(min(k_vals) - 0.2, max(k_vals) + 0.2, 100)\n",
    "t_theory = c_fit * (b_fit ** k_theory)\n",
    "ax2.semilogy(k_theory, t_theory, '--', color='#E53935', linewidth=2, \n",
    "             label=f'Fit: {c_fit:.0f} × {b_fit:.2f}^k (R²={r_sq:.3f})', zorder=2)\n",
    "\n",
    "# Reference O(2^k) line (normalized to first data point)\n",
    "c_ref = means[0] / (2 ** k_vals[0])\n",
    "t_ref = c_ref * (2 ** k_theory)\n",
    "ax2.semilogy(k_theory, t_ref, ':', color='#4CAF50', linewidth=2, alpha=0.7,\n",
    "             label='Reference: O(2^k)', zorder=1)\n",
    "\n",
    "ax2.set_xlabel('Number of Destinations (k)')\n",
    "ax2.set_ylabel('Time (ms) - log scale')\n",
    "ax2.set_title(f'Routing Scaling: O({b_fit:.2f}^k) ≈ O(2^k)')\n",
    "ax2.set_xticks(k_vals)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "if has_validation and axes[1][0] is not None:\n",
    "    # Plot 3: Validation latency distribution\n",
    "    ax3 = axes[1][0]\n",
    "    times = [r.validation_time_ms for r in validation_results]\n",
    "    ax3.hist(times, bins=10, color='#FF9800', alpha=0.8, edgecolor='white')\n",
    "    ax3.axvline(validation_stats['mean'], color='red', linestyle='--', label=f\"Mean: {validation_stats['mean']:.0f}ms\")\n",
    "    ax3.set_xlabel('Validation Time (ms)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Part B: Validation Latency Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "if has_pipeline and axes[1][1] is not None:\n",
    "    # Plot 4: Pipeline breakdown\n",
    "    ax4 = axes[1][1]\n",
    "    k_vals_p = sorted(pipeline_stats.keys())\n",
    "    routing_p = [pipeline_stats[k]['routing_mean'] for k in k_vals_p]\n",
    "    validation_p = [pipeline_stats[k]['validation_mean'] for k in k_vals_p]\n",
    "    x = np.arange(len(k_vals_p))\n",
    "    ax4.bar(x, routing_p, 0.6, label='Routing', color='#2196F3', alpha=0.8)\n",
    "    ax4.bar(x, validation_p, 0.6, bottom=routing_p, label='Validation', color='#FF9800', alpha=0.8)\n",
    "    ax4.set_xlabel('Number of Destinations (k)')\n",
    "    ax4.set_ylabel('Time (ms)')\n",
    "    ax4.set_title('Part C: Full Pipeline Breakdown')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(k_vals_p)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: benchmark_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime as dt\n",
    "\n",
    "export_data = {\n",
    "    'timestamp': dt.now().isoformat(),\n",
    "    'benchmarks': {\n",
    "        'routing': {\n",
    "            'config': {\n",
    "                'max_k': routing_config.max_k,\n",
    "                'combos_per_k': routing_config.combos_per_k,\n",
    "                'runs_per_combo': routing_config.runs_per_combo,\n",
    "            },\n",
    "            'statistics': {str(k): v for k, v in routing_stats.items()},\n",
    "            'raw_results': [\n",
    "                {'k': r.k, 'destinations': list(r.destinations), 'time_ms': r.time_ms, 'num_routes': r.num_routes}\n",
    "                for r in routing_results\n",
    "            ],\n",
    "        },\n",
    "        'validation': {\n",
    "            'statistics': validation_stats,\n",
    "            'raw_results': [\n",
    "                {'route_index': r.route_index, 'time_ms': r.validation_time_ms, 'status': r.status, 'confidence': r.confidence}\n",
    "                for r in validation_results\n",
    "            ] if validation_results else [],\n",
    "        } if validation_stats else None,\n",
    "        'pipeline': {\n",
    "            'statistics': {str(k): v for k, v in pipeline_stats.items()},\n",
    "            'raw_results': [\n",
    "                {'k': r.k, 'routing_ms': r.routing_time_ms, 'validation_ms': r.validation_time_ms, 'total_ms': r.total_time_ms}\n",
    "                for r in pipeline_results\n",
    "            ],\n",
    "        } if pipeline_stats else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "output_file = Path('benchmark_results.json')\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"Results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nPart A: Routing Benchmark\")\n",
    "print(\"-\" * 40)\n",
    "for k in sorted(routing_stats.keys()):\n",
    "    s = routing_stats[k]\n",
    "    print(f\"  k={k}: {s['mean']:.0f}ms (+/-{s['std']:.0f}ms)\")\n",
    "\n",
    "if validation_stats:\n",
    "    print(\"\\nPart B: Validation Benchmark\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Mean latency: {validation_stats['mean']:.0f}ms\")\n",
    "    print(f\"  Std dev: {validation_stats['std']:.0f}ms\")\n",
    "    print(f\"  Bookable rate: {validation_stats['bookable_rate']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nPart B: Validation Benchmark - SKIPPED (no API token)\")\n",
    "\n",
    "if pipeline_stats:\n",
    "    print(\"\\nPart C: Full Pipeline Benchmark\")\n",
    "    print(\"-\" * 40)\n",
    "    for k in sorted(pipeline_stats.keys()):\n",
    "        s = pipeline_stats[k]\n",
    "        print(f\"  k={k}: routing={s['routing_mean']:.0f}ms + validation={s['validation_mean']:.0f}ms = {s['total_mean']:.0f}ms\")\n",
    "else:\n",
    "    print(\"\\nPart C: Full Pipeline Benchmark - SKIPPED (no API token)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Files generated:\")\n",
    "print(\"  - benchmark_results.png\")\n",
    "print(\"  - benchmark_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
